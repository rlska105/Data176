{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization():\n",
    "    def __init__(self, R, k, learning_rate, reg_param, epochs, verbose=False):\n",
    "\n",
    "        self._R = R\n",
    "        self._num_users, self._num_items = R.shape\n",
    "        self._k = k\n",
    "        self._learning_rate = learning_rate\n",
    "        self._reg_param = reg_param\n",
    "        self._epochs = epochs\n",
    "        self._verbose = verbose\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        training Matrix Factorization : Update matrix latent weight and bias\n",
    "\n",
    "        참고: self._b에 대한 설명\n",
    "        - global bias: input R에서 평가가 매겨진 rating의 평균값을 global bias로 사용\n",
    "        - 정규화 기능. 최종 rating에 음수가 들어가는 것 대신 latent feature에 음수가 포함되도록 해줌.\n",
    "\n",
    "        :return: training_process\n",
    "        \"\"\"\n",
    "\n",
    "        # init latent features\n",
    "        self._P = np.random.normal(size=(self._num_users, self._k))\n",
    "        self._Q = np.random.normal(size=(self._num_items, self._k))\n",
    "\n",
    "        # init biases\n",
    "        self._b_P = np.zeros(self._num_users)\n",
    "        self._b_Q = np.zeros(self._num_items)\n",
    "        self._b = np.mean(self._R[np.where(self._R != 0)])\n",
    "\n",
    "        # train while epochs\n",
    "        self._training_process = []\n",
    "        for epoch in range(self._epochs):\n",
    "\n",
    "            # rating이 존재하는 index를 기준으로 training\n",
    "            for i in range(self._num_users):\n",
    "                for j in range(self._num_items):\n",
    "                    if self._R[i, j] > 0:\n",
    "                        self.gradient_descent(i, j, self._R[i, j])\n",
    "            cost = self.cost()\n",
    "            self._training_process.append((epoch, cost))\n",
    "\n",
    "            # print status\n",
    "            if self._verbose == True and ((epoch + 1) % 10 == 0):\n",
    "                print(\"Iteration: %d ; cost = %.4f\" % (epoch + 1, cost))\n",
    "\n",
    "\n",
    "    def cost(self):\n",
    "        \"\"\"\n",
    "        compute root mean square error\n",
    "        :return: rmse cost\n",
    "        \"\"\"\n",
    "\n",
    "        # xi, yi: R[xi, yi]는 nonzero인 value를 의미한다.\n",
    "        # 참고: http://codepractice.tistory.com/90\n",
    "        xi, yi = self._R.nonzero()\n",
    "        predicted = self.get_complete_matrix()\n",
    "        cost = 0\n",
    "        for x, y in zip(xi, yi):\n",
    "            cost += pow(self._R[x, y] - predicted[x, y], 2)\n",
    "        return np.sqrt(cost) / len(xi)\n",
    "\n",
    "\n",
    "    def gradient(self, error, i, j):\n",
    "        \"\"\"\n",
    "        gradient of latent feature for GD\n",
    "\n",
    "        :param error: rating - prediction error\n",
    "        :param i: user index\n",
    "        :param j: item index\n",
    "        :return: gradient of latent feature tuple\n",
    "        \"\"\"\n",
    "\n",
    "        dp = (error * self._Q[j, :]) - (self._reg_param * self._P[i, :])\n",
    "        dq = (error * self._P[i, :]) - (self._reg_param * self._Q[j, :])\n",
    "        return dp, dq\n",
    "\n",
    "\n",
    "    def gradient_descent(self, i, j, rating):\n",
    "        \"\"\"\n",
    "        graident descent function\n",
    "\n",
    "        :param i: user index of matrix\n",
    "        :param j: item index of matrix\n",
    "        :param rating: rating of (i,j)\n",
    "        \"\"\"\n",
    "\n",
    "        # get error\n",
    "        prediction = self.get_prediction(i, j)\n",
    "        error = rating - prediction\n",
    "\n",
    "        # update biases\n",
    "        self._b_P[i] += self._learning_rate * (error - self._reg_param * self._b_P[i])\n",
    "        self._b_Q[j] += self._learning_rate * (error - self._reg_param * self._b_Q[j])\n",
    "\n",
    "        # update latent feature\n",
    "        dp, dq = self.gradient(error, i, j)\n",
    "        self._P[i, :] += self._learning_rate * dp\n",
    "        self._Q[j, :] += self._learning_rate * dq\n",
    "\n",
    "\n",
    "    def get_prediction(self, i, j):\n",
    "        \"\"\"\n",
    "        get predicted rating: user_i, item_j\n",
    "        :return: prediction of r_ij\n",
    "        \"\"\"\n",
    "        return self._b + self._b_P[i] + self._b_Q[j] + self._P[i, :].dot(self._Q[j, :].T)\n",
    "\n",
    "\n",
    "    def get_complete_matrix(self):\n",
    "        \"\"\"\n",
    "        computer complete matrix PXQ + P.bias + Q.bias + global bias\n",
    "\n",
    "        - PXQ 행렬에 b_P[:, np.newaxis]를 더하는 것은 각 열마다 bias를 더해주는 것\n",
    "        - b_Q[np.newaxis:, ]를 더하는 것은 각 행마다 bias를 더해주는 것\n",
    "        - b를 더하는 것은 각 element마다 bias를 더해주는 것\n",
    "\n",
    "        - newaxis: 차원을 추가해줌. 1차원인 Latent들로 2차원의 R에 행/열 단위 연산을 해주기위해 차원을 추가하는 것.\n",
    "\n",
    "        :return: complete matrix R^\n",
    "        \"\"\"\n",
    "        return self._b + self._b_P[:, np.newaxis] + self._b_Q[np.newaxis:, ] + self._P.dot(self._Q.T)\n",
    "\n",
    "\n",
    "    def print_results(self):\n",
    "        \"\"\"\n",
    "        print fit results\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"User Latent P:\")\n",
    "        print(self._P)\n",
    "        print(\"Item Latent Q:\")\n",
    "        print(self._Q.T)\n",
    "        print(\"P x Q:\")\n",
    "        print(self._P.dot(self._Q.T))\n",
    "        print(\"bias:\")\n",
    "        print(self._b)\n",
    "        print(\"User Latent bias:\")\n",
    "        print(self._b_P)\n",
    "        print(\"Item Latent bias:\")\n",
    "        print(self._b_Q)\n",
    "        print(\"Final R matrix:\")\n",
    "        print(self.get_complete_matrix())\n",
    "        print(\"Final RMSE:\")\n",
    "        print(self._training_process[self._epochs-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 ; cost = 0.2711\n",
      "Iteration: 20 ; cost = 0.2105\n",
      "Iteration: 30 ; cost = 0.1740\n",
      "Iteration: 40 ; cost = 0.1455\n",
      "Iteration: 50 ; cost = 0.1212\n",
      "Iteration: 60 ; cost = 0.1000\n",
      "Iteration: 70 ; cost = 0.0811\n",
      "Iteration: 80 ; cost = 0.0646\n",
      "Iteration: 90 ; cost = 0.0506\n",
      "Iteration: 100 ; cost = 0.0391\n",
      "Iteration: 110 ; cost = 0.0301\n",
      "Iteration: 120 ; cost = 0.0231\n",
      "Iteration: 130 ; cost = 0.0179\n",
      "Iteration: 140 ; cost = 0.0140\n",
      "Iteration: 150 ; cost = 0.0111\n",
      "Iteration: 160 ; cost = 0.0090\n",
      "Iteration: 170 ; cost = 0.0075\n",
      "Iteration: 180 ; cost = 0.0064\n",
      "Iteration: 190 ; cost = 0.0056\n",
      "Iteration: 200 ; cost = 0.0050\n",
      "Iteration: 210 ; cost = 0.0046\n",
      "Iteration: 220 ; cost = 0.0043\n",
      "Iteration: 230 ; cost = 0.0041\n",
      "Iteration: 240 ; cost = 0.0040\n",
      "Iteration: 250 ; cost = 0.0038\n",
      "Iteration: 260 ; cost = 0.0038\n",
      "Iteration: 270 ; cost = 0.0037\n",
      "Iteration: 280 ; cost = 0.0037\n",
      "Iteration: 290 ; cost = 0.0036\n",
      "Iteration: 300 ; cost = 0.0036\n",
      "User Latent P:\n",
      "[[ 1.64733704 -0.25459999 -0.16899757]\n",
      " [-0.71041977 -0.09297586 -0.0120289 ]\n",
      " [ 2.0925114   1.14860816  1.62940162]\n",
      " [-0.2223072  -0.99513325  1.19440319]\n",
      " [ 0.52523645  0.38925219  1.00320619]\n",
      " [-0.85249466  0.76470342  0.25043952]\n",
      " [ 0.05515314  0.22927457 -0.18399671]]\n",
      "Item Latent Q:\n",
      "[[-0.51466989  1.38161334  0.13993108  0.04436247  0.70523912]\n",
      " [ 0.8103158  -0.37059321 -0.78883804  0.15556772 -1.13859822]\n",
      " [-1.18416216 -1.33792116  0.75711269  1.38044385  0.14967913]]\n",
      "P x Q:\n",
      "[[-0.85402064  2.59644128  0.30340161 -0.19981925  1.42635821]\n",
      " [ 0.30453602 -0.93097549 -0.03517414 -0.06258524 -0.39695414]\n",
      " [-2.075693    0.28536438  0.62038221  2.52081278  0.41180511]\n",
      " [-2.10632442 -1.53637027  1.65818908  1.48413383  1.15505444]\n",
      " [-1.14286499 -0.76079133  0.5259801   1.46872568  0.07737447]\n",
      " [ 0.76184359 -1.79628023 -0.53290671  0.4268621  -1.43441697]\n",
      " [ 0.37528109  0.23740581 -0.31244911 -0.21588268 -0.24969593]]\n",
      "bias:\n",
      "2.590909090909091\n",
      "User Latent bias:\n",
      "[-0.8958217  -1.04285535  0.33523542  0.37622825  0.39384386  1.44467736\n",
      " -0.89463919]\n",
      "Item Latent bias:\n",
      "[ 0.16087693 -1.21640452  1.47972079 -0.46652761 -0.132287  ]\n",
      "Final R matrix:\n",
      "[[ 1.00194369  3.07512415  3.47820979  1.02874053  2.9891586 ]\n",
      " [ 2.01346669 -0.59932627  2.99260039  1.01894089  1.01881259]\n",
      " [ 1.01132845  1.99510438  5.02624752  4.98042969  3.20566262]\n",
      " [ 1.02168985  0.21436255  6.10504722  3.98474356  3.98990478]\n",
      " [ 2.0027649   1.0075571   4.99045385  3.98695102  2.92984041]\n",
      " [ 4.95830697  1.0229017   4.98240053  3.99592094  2.46888247]\n",
      " [ 2.23242792  0.71727119  2.86354158  1.01385961  1.31428696]]\n",
      "Final RMSE:\n",
      "0.0036204063913223765\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    R = np.array([\n",
    "        [1, 0, 0, 1, 3],\n",
    "        [2, 0, 3, 1, 1],\n",
    "        [1, 2, 0, 5, 0],\n",
    "        [1, 0, 0, 4, 4],\n",
    "        [2, 1, 5, 4, 0],\n",
    "        [5, 1, 5, 4, 0],\n",
    "        [0, 0, 0, 1, 0],\n",
    "    ])\n",
    "\n",
    "\n",
    "factorizer = MatrixFactorization(R, k=3, learning_rate=0.01, reg_param=0.01, epochs=300, verbose=True)\n",
    "factorizer.fit()\n",
    "factorizer.print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
